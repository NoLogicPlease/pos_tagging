{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Assignment1-2021.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WeCeITXoxLf"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "**Due to**: 20/12/2021 (dd/mm/yyyy)\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Summary**: Part-of Speech (POS) tagging as Sequence Labelling using Recurrent Neural Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4_wqPdlBcKS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Intro\n",
    "\n",
    "In this assignment  we will ask you to perform POS tagging using neural architectures\n",
    "\n",
    "You are asked to follow these steps:\n",
    "1. Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "2. Embed the words using GloVe embeddings\n",
    "3. Create a baseline model, using a simple neural architecture\n",
    "4. Experiment doing small modifications to the baseline model, choose hyperparameters using the validation set\n",
    "5. Evaluate your two best model\n",
    "6. Analyze the errors of your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 13:23:34.558620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-24 13:23:34.558690: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib import request\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "WINDOW_LENGTH = 5\n",
    "EMBEDDING_SIZE = 50\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset... (it may take a while...)\n",
      "Extraction completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 882.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsj_0096.dp\n",
      "The following were barred or , where noted , suspended and consented to findings without admitting or denying wrongdoing : Edward L. Cole , Jackson , Miss. , $ 10,000 fine ; Rita Rae Cross , Denver , $ 2,500 fine and 30-day suspension ; Thomas Richard Meinders , Colorado Springs , Colo. , $ 2,000 fine , five-day suspension and eight-month suspension as a principal ; Ronald A. Cutrer , Baton Rouge , La. , $ 15,000 fine and one-month suspension ; Karl Grant Hale , Midvale , Utah , $ 15,000 fine ; Clinton P. Hayne , New Orleans , $ 7,500 fine and one-week suspension ; Richard M. Kane , Coconut Creek , Fla. , $ 250,000 fine ; John B. Merrick , Aurora , Colo. , $ 1,000 fine and 10-day suspension ; John P. Miller , Baton Rouge , $ 2,000 fine and two-week suspension ; Randolph K. Pace , New York , $ 10,000 fine and 90-day suspension ; Brian D. Pitcher , New Providence , N.J. , $ 30,000 fine ; Wayne A. Russo , Bridgeville , Pa. , $ 4,000 fine and 15-day suspension ; Orville Leroy Sandberg , Aurora , Colo. , $ 3,500 fine and 10-day suspension ; Richard T. Marchese , Las Vegas , Nev. , $ 5,000 and one-year suspension ; Eric G. Monchecourt , Las Vegas , $ 5,000 and one-year suspension ; and Robert Gerhard Smith , Carson City , Nev. , two-year suspension . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [00:00, 897.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                   text  \\\n0     pierre vinken , 61 years old , will join the b...   \n1     mr. vinken is chairman of elsevier n.v. , the ...   \n2     rudolph agnew , 55 years old and former chairm...   \n3     a form of asbestos once used to make kent ciga...   \n4     the asbestos fiber , crocidolite , is unusuall...   \n...                                                 ...   \n3909  they also said that more than a dozen presiden...   \n3910  sen. kennedy said in a separate statement that...   \n3911  trinity industries inc. said it reached a prel...   \n3912                        terms were n't disclosed .    \n3913  trinity said it plans to begin delivery in the...   \n\n                                             POStagging  split  \n0     nnp nnp , cd nns jj , md vb dt nn in dt jj nn ...  train  \n1          nnp nnp vbz nn in nnp nnp , dt nnp vbg nn .   train  \n2     nnp nnp , cd nns jj cc jj nn in nnp nnp nnp nn...  train  \n3     dt nn in nn rb vbn to vb nnp nn nns vbz vbn dt...  train  \n4     dt nn nn , nn , vbz rb jj in prp vbz dt nns , ...  train  \n...                                                 ...    ...  \n3909  prp rb vbd in jjr in dt nn nns vbp vbn in jj n...   test  \n3910  nnp nnp vbd in dt jj nn in prp vbz nn to vb dt...   test  \n3911  nnp nnps nnp vbd prp vbd dt jj nn to vb cd nn ...   test  \n3912                                  nns vbd rb vbn .    test  \n3913   nnp vbd prp vbz to vb nn in dt jj nn in jj nn .    test  \n\n[3914 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>POStagging</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pierre vinken , 61 years old , will join the b...</td>\n      <td>nnp nnp , cd nns jj , md vb dt nn in dt jj nn ...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mr. vinken is chairman of elsevier n.v. , the ...</td>\n      <td>nnp nnp vbz nn in nnp nnp , dt nnp vbg nn .</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rudolph agnew , 55 years old and former chairm...</td>\n      <td>nnp nnp , cd nns jj cc jj nn in nnp nnp nnp nn...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>a form of asbestos once used to make kent ciga...</td>\n      <td>dt nn in nn rb vbn to vb nnp nn nns vbz vbn dt...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>the asbestos fiber , crocidolite , is unusuall...</td>\n      <td>dt nn nn , nn , vbz rb jj in prp vbz dt nns , ...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3909</th>\n      <td>they also said that more than a dozen presiden...</td>\n      <td>prp rb vbd in jjr in dt nn nns vbp vbn in jj n...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3910</th>\n      <td>sen. kennedy said in a separate statement that...</td>\n      <td>nnp nnp vbd in dt jj nn in prp vbz nn to vb dt...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3911</th>\n      <td>trinity industries inc. said it reached a prel...</td>\n      <td>nnp nnps nnp vbd prp vbd dt jj nn to vb cd nn ...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3912</th>\n      <td>terms were n't disclosed .</td>\n      <td>nns vbd rb vbn .</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>3913</th>\n      <td>trinity said it plans to begin delivery in the...</td>\n      <td>nnp vbd prp vbz to vb nn in dt jj nn in jj nn .</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>3914 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "\n",
    "dataset_path_zip = os.path.join(dataset_folder, \"dependency_treebank.zip\")\n",
    "\n",
    "\n",
    "def download_dataset(download_path: str, url: str):\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        request.urlretrieve(url, download_path)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "\n",
    "def extract_dataset(download_path: str, extract_path: str):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as loaded_zip:\n",
    "        loaded_zip.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")\n",
    "\n",
    "\n",
    "download_dataset(dataset_path_zip, url)\n",
    "extract_dataset(dataset_path_zip, dataset_folder)\n",
    "dataset_path = os.path.join(dataset_folder, \"dependency_treebank\")\n",
    "\n",
    "\n",
    "def encode_dataset(dataset_path: str, debug: bool = True) -> (pd.DataFrame, int):\n",
    "    dataframe_rows = []\n",
    "    s_lengths = []\n",
    "    for index, file in tqdm(enumerate(sorted(os.listdir(dataset_path)))):\n",
    "        file_name = os.path.join(dataset_path, file)\n",
    "        with open(file_name) as f:\n",
    "            lines = f.readlines()\n",
    "        full_file = ''.join(lines)  # since lines is a list we use a single string called full_file\n",
    "        full_file = re.sub(r'(\\t\\d+)', '', full_file)  # remove numbers from each lines of dataset\n",
    "        full_file = re.sub(r'(\\t)', ' ', full_file)  # replace \\t with a space\n",
    "        sentences = full_file.split('\\n\\n')\n",
    "        for s in sentences:  #separate all words from their tags\n",
    "            text = ''.join(re.findall(r'.+ ', s))\n",
    "            labels = ''.join(re.findall(r' .+', s))\n",
    "            labels = re.sub(r' (.+)', r'\\1 ', labels)\n",
    "            labels = re.sub('\\n', ' ', labels)\n",
    "            s_lengths.append(len(labels.split(' ')))\n",
    "            if len(labels.split(' ')) == 250:\n",
    "                print(file)\n",
    "                print(text)\n",
    "            # split into train, val and test\n",
    "            if index <= 100:\n",
    "                split = 'train'\n",
    "            elif 100 < index <= 150:\n",
    "                split = 'val'\n",
    "            else:\n",
    "                split = 'test'\n",
    "\n",
    "            # create a single row of dataframe\n",
    "            dataframe_row = {\n",
    "                \"text\": text,\n",
    "                \"POStagging\": labels,\n",
    "                \"split\": split,\n",
    "            }\n",
    "            dataframe_rows.append(dataframe_row)\n",
    "\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"text\",\n",
    "             \"POStagging\",\n",
    "             \"split\"]]\n",
    "    dataframe_path = os.path.join(dataset_folder, \"dependency_treebank_df.pkl\")\n",
    "    df.to_pickle(dataframe_path)\n",
    "    return df, max(s_lengths)\n",
    "\n",
    "\n",
    "df, max_seq_len = encode_dataset(dataset_path)\n",
    "df['POStagging'] = df['POStagging'].str.lower()\n",
    "df['text'] = df['text'].str.lower()\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer...\n",
      "Fit completed!\n",
      "Loading embedding model! It may take a while...\n",
      "Checking OOV terms...\n",
      "Total OOV terms: 363 (0.05%)\n",
      "Building the embedding matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7482/7482 [00:00<00:00, 213937.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Fitting tokenizer...\n",
      "Fit completed!\n",
      "Loading embedding model! It may take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking OOV terms...\n",
      "Total OOV terms: 214 (0.04%)\n",
      "Building the embedding matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5347/5347 [00:00<00:00, 247633.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 69/5347 [00:00<00:00, 66362.53it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key '-rrb-' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_27700/4166348594.py\u001B[0m in \u001B[0;36mconcatenate_embeddings\u001B[0;34m(emb_model1, emb_model2, word_to_index_model2, emb_mat1)\u001B[0m\n\u001B[1;32m     53\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m             \u001B[0mcheck\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0memb_model1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     55\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mKeyError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key_or_keys)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 397\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey_or_keys\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 397\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey_or_keys\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mget_vector\u001B[0;34m(self, key, norm)\u001B[0m\n\u001B[1;32m    437\u001B[0m         \"\"\"\n\u001B[0;32m--> 438\u001B[0;31m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    439\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mget_index\u001B[0;34m(self, key, default)\u001B[0m\n\u001B[1;32m    411\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 412\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Key '{key}' not present\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    413\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key '-rrb-' not present\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_27700/4166348594.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[0mv2_tokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild_vocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"split\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"val\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'text'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m print(concatenate_embeddings(v1_tokenizer.embedding_model,\n\u001B[0m\u001B[1;32m    148\u001B[0m                              \u001B[0mv2_tokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding_model\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m                              \u001B[0mv2_tokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_27700/4166348594.py\u001B[0m in \u001B[0;36mconcatenate_embeddings\u001B[0;34m(emb_model1, emb_model2, word_to_index_model2, emb_mat1)\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0mcheck\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0memb_model1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mKeyError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m             \u001B[0memb_mat1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memb_mat1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0memb_model2\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0memb_mat1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, key_or_keys)\u001B[0m\n\u001B[1;32m    395\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey_or_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 397\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey_or_keys\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    399\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    395\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey_or_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 397\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mvstack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mkey_or_keys\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    398\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    399\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mget_vector\u001B[0;34m(self, key, norm)\u001B[0m\n\u001B[1;32m    436\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    437\u001B[0m         \"\"\"\n\u001B[0;32m--> 438\u001B[0;31m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    439\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    440\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfill_norms\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/share/virtualenvs/Assignment1-Su6BFbl9/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001B[0m in \u001B[0;36mget_index\u001B[0;34m(self, key, default)\u001B[0m\n\u001B[1;32m    410\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mdefault\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    411\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 412\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Key '{key}' not present\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    413\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    414\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnorm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: \"Key '-rrb-' not present\""
     ]
    }
   ],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_vocabulary = set(embedding_model.index_to_key)\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    print(\"Total OOV terms: {0} ({1:.2f}%)\".format(len(list(oov)), float(len(list(oov))) / len(word_listing)))\n",
    "    return list(oov)\n",
    "\n",
    "\n",
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx,\n",
    "                           vocab_size: int,\n",
    "                           oov_terms) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def concatenate_embeddings(emb_model1, emb_model2, word_to_index_model2, emb_mat1):\n",
    "    emb_model1 = copy.deepcopy(emb_model1)\n",
    "    emb_mat1 = copy.deepcopy(emb_mat1)\n",
    "    for word in tqdm(word_to_index_model2.items()):\n",
    "        try:\n",
    "            check = emb_model1[word]\n",
    "        except (KeyError, TypeError):\n",
    "            emb_mat1 = np.vstack((emb_mat1, emb_model2[word]))\n",
    "    return emb_mat1\n",
    "\n",
    "\n",
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    A simple high-level wrapper for the Keras tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
    "                 embedding_model_type=None, tokenizer_args=None):\n",
    "        if build_embedding_matrix:\n",
    "            assert embedding_model_type is not None\n",
    "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
    "\n",
    "        self.build_embedding_matrix = build_embedding_matrix\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.embedding_model = None\n",
    "        self.embedding_matrix = None\n",
    "        self.vocab = None\n",
    "\n",
    "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
    "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
    "\n",
    "        self.tokenizer_args = tokenizer_args\n",
    "\n",
    "    def build_vocab(self, data, **kwargs):\n",
    "        print('Fitting tokenizer...')\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
    "        self.tokenizer.fit_on_texts(data)\n",
    "        print('Fit completed!')\n",
    "\n",
    "        self.vocab = self.tokenizer.word_index\n",
    "\n",
    "        if self.build_embedding_matrix:\n",
    "            print('Loading embedding model! It may take a while...')\n",
    "            self.embedding_model = gloader.load(f\"glove-wiki-gigaword-{self.embedding_dimension}\")\n",
    "\n",
    "            print('Checking OOV terms...')\n",
    "            self.oov_terms = check_OOV_terms(embedding_model=self.embedding_model,\n",
    "                                             word_listing=list(self.vocab.keys()))\n",
    "\n",
    "            print('Building the embedding matrix...')\n",
    "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
    "                                                           word_to_idx=self.vocab,\n",
    "                                                           vocab_size=len(self.vocab) + 1,\n",
    "                                                           embedding_dimension=self.embedding_dimension,\n",
    "                                                           oov_terms=self.oov_terms)\n",
    "            print('Done!')\n",
    "\n",
    "    def get_info(self):\n",
    "        return {\n",
    "            'build_embedding_matrix': self.build_embedding_matrix,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'embedding_model_type': self.embedding_model_type,\n",
    "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
    "            'embedding_model': self.embedding_model,\n",
    "            'vocab_size': len(self.vocab) + 1,\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if type(tokens) == str:\n",
    "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
    "        else:\n",
    "            return self.tokenizer.texts_to_sequences(tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.tokenizer.sequences_to_texts(ids)\n",
    "\n",
    "\n",
    "tokenizer_args = {\n",
    "    'oov_token': 1,  # The vocabulary id for unknown terms during text conversion\n",
    "    'lower': True,\n",
    "    'filters': ''\n",
    "}\n",
    "v1_tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
    "                           build_embedding_matrix=True,\n",
    "                           embedding_dimension=EMBEDDING_SIZE,\n",
    "                           embedding_model_type=\"glove\")\n",
    "v2_tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
    "                           build_embedding_matrix=True,\n",
    "                           embedding_dimension=EMBEDDING_SIZE,\n",
    "                           embedding_model_type=\"glove\")\n",
    "\n",
    "v1_tokenizer.build_vocab(df[df[\"split\"] == \"train\"]['text'].values)\n",
    "v2_tokenizer.build_vocab(df[df[\"split\"] == \"val\"]['text'].values)\n",
    "\n",
    "print(concatenate_embeddings(v1_tokenizer.embedding_model,\n",
    "                             v2_tokenizer.embedding_model,\n",
    "                             v2_tokenizer.vocab,\n",
    "                             v1_tokenizer.embedding_matrix))\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_info = v2_tokenizer.get_info()\n",
    "\n",
    "print('Tokenizer info: ', tokenizer_info)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TwoWay:\n",
    "    def __init__(self):\n",
    "        self.d = {}\n",
    "\n",
    "    def add(self, k, v):\n",
    "        self.d[k] = v\n",
    "        self.d[v] = k\n",
    "\n",
    "    def remove(self, k):\n",
    "        self.d.pop(self.d.pop(k))\n",
    "\n",
    "    def get(self, k):\n",
    "        return self.d[k]\n",
    "\n",
    "\n",
    "def create_trainable(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Converts input text sequences using a given tokenizer\n",
    "\n",
    "    :param texts: either a list or numpy ndarray of strings\n",
    "    :tokenizer: an instantiated tokenizer\n",
    "\n",
    "    :return\n",
    "        text_ids: a nested list on token indices\n",
    "    \"\"\"\n",
    "    text_ids = tokenizer.convert_tokens_to_ids(dataset['text'])\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    label_tokenizer = TwoWay()\n",
    "\n",
    "    label_id = 1\n",
    "    for sen in dataset[\"POStagging\"]:\n",
    "        for label in sen.split():\n",
    "            try:\n",
    "                check_label = label_tokenizer.d[label]\n",
    "            except KeyError:\n",
    "                label_tokenizer.add(label_id, label)\n",
    "                label_id += 1\n",
    "\n",
    "    for sen, tagging in zip(text_ids, dataset[\"POStagging\"]):\n",
    "        tmp = [0] * (max_seq_len - len(sen)) + sen\n",
    "        x_train.append(tmp)\n",
    "        tmp = [0] * (max_seq_len - len(sen)) + [label_tokenizer.get(e) for e in tagging.split()]\n",
    "        y_train.append(tmp)\n",
    "    return x_train, y_train, label_tokenizer\n",
    "\n",
    "\n",
    "# Train, Val\n",
    "x_train, y_train, label_tok = create_trainable(df[df[\"split\"] == \"train\"], tokenizer)\n",
    "x_val, y_val, label_tok = create_trainable(df[df[\"split\"] == \"val\"], tokenizer)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "num_labels = 46\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 46)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, 46)\n",
    "\n",
    "print(x_train.shape)\n",
    "# per vedere la conversione\n",
    "#tokens = tokenizer.convert_ids_to_tokens(list(text_ids)) #verify token\n",
    "#print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model(compile_info: dict) -> keras.Model:\n",
    "    bidirect_model = keras.models.Sequential()\n",
    "    bidirect_model.add(layers.Embedding(input_dim=len(tokenizer.vocab) + 1,\n",
    "                                        output_dim=EMBEDDING_SIZE,\n",
    "                                        input_length=max_seq_len,\n",
    "                                        mask_zero=True,\n",
    "                                        weights=tokenizer.embedding_matrix if tokenizer.embedding_matrix is None else [\n",
    "                                            tokenizer.embedding_matrix],\n",
    "                                        trainable=False\n",
    "                                        ))\n",
    "    bidirect_model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
    "    bidirect_model.add(layers.TimeDistributed(layers.Dense(num_labels, activation=\"softmax\")))\n",
    "\n",
    "    bidirect_model.compile(**compile_info)\n",
    "    bidirect_model.summary()\n",
    "    return bidirect_model\n",
    "\n",
    "\n",
    "compile_info = {\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'loss': 'categorical_crossentropy',\n",
    "    'metrics': ['accuracy']\n",
    "}\n",
    "\n",
    "bidirect_model = create_model(compile_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def show_history(history: keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    Shows training history data stored by the History Keras callback\n",
    "\n",
    "    :param history: History Keras callback\n",
    "    \"\"\"\n",
    "\n",
    "    history_data = history.history\n",
    "    print(\"Displaying the following history keys: \", history_data.keys())\n",
    "\n",
    "    for key, value in history_data.items():\n",
    "        if not key.startswith('val'):\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.set_title(key)\n",
    "            ax.plot(value)\n",
    "            if 'val_{}'.format(key) in history_data:\n",
    "                ax.plot(history_data['val_{}'.format(key)])\n",
    "            else:\n",
    "                print(\"Couldn't find validation values for metric: \", key)\n",
    "\n",
    "            ax.set_ylabel(key)\n",
    "            ax.set_xlabel('epoch')\n",
    "            ax.legend(['train', 'val'], loc='best')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: keras.Model,\n",
    "                x_train: np.ndarray,\n",
    "                y_train: np.ndarray,\n",
    "                x_val: np.ndarray,\n",
    "                y_val: np.ndarray,\n",
    "                training_info: dict):\n",
    "    \"\"\"\n",
    "    Training routine for the Keras model.\n",
    "    At the end of the training, retrieved History data is shown.\n",
    "\n",
    "    :param model: Keras built model\n",
    "    :param x_train: training data in np.ndarray format\n",
    "    :param y_train: training labels in np.ndarray format\n",
    "    :param x_val: validation data in np.ndarray format\n",
    "    :param y_val: validation labels in np.ndarray format\n",
    "    :param training_info: dictionary storing model fit() argument information\n",
    "\n",
    "    :return\n",
    "        model: trained Keras model\n",
    "    \"\"\"\n",
    "    print(\"Start training! \\nParameters: {}\".format(training_info))\n",
    "    history = model.fit(x=x_train, y=y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        **training_info)\n",
    "    print(\"Training completed! Showing history...\")\n",
    "\n",
    "    show_history(history)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_data(model: keras.Model,\n",
    "                 x: np.ndarray,\n",
    "                 prediction_info: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inference routine of a given input set of examples\n",
    "\n",
    "    :param model: Keras built and possibly trained model\n",
    "    :param x: input set of examples in np.ndarray format\n",
    "    :param prediction_info: dictionary storing model predict() argument information\n",
    "\n",
    "    :return\n",
    "        predictions: predicted labels in np.ndarray format\n",
    "    \"\"\"\n",
    "\n",
    "    print('Starting prediction: \\n{}'.format(prediction_info))\n",
    "    print('Predicting on {} samples'.format(x.shape[0]))\n",
    "\n",
    "    predictions = model.predict(x, **prediction_info)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=10)]\n",
    "}\n",
    "model = train_model(model=bidirect_model, x_train=x_train, y_train=y_train,\n",
    "                    x_val=x_val, y_val=y_val, training_info=training_info)\n",
    "\n",
    "# t-SNE\n",
    "'''\n",
    "reduced_embedding_tSNE = reduce_tSNE(embedding_matrix)\n",
    "visualize_embeddings(reduced_embedding_tSNE)\n",
    "visualize_embeddings(reduced_embedding_tSNE,\n",
    "                     ['good', 'love', 'beautiful'],\n",
    "                     word_to_idx)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Task**: given a corpus of documents, predict the POS tag for each word\n",
    "\n",
    "**Corpus**:\n",
    "Ignore the numeric value in the third column, use only the words/symbols and its label. \n",
    "The corpus is available at:\n",
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "\n",
    "**Splits**: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "**Features**: you MUST use GloVe embeddings as the only input features to the model.\n",
    "\n",
    "**Splitting**: you can decide to split documents into sentences or not, the choice is yours.\n",
    "\n",
    "**I/O structure**: The input data will have three dimensions:\n",
    "1-documents/sentences, 2-token, 3-features; for the output there are 2 possibilities: if you use one-hot encoding it will be 1-documents/sentences, 2-token labels, 3-classes, if you use a single integer that indicates the number of the class it will be 1-documents/sentences, 2-token labels.\n",
    "\n",
    "**Baseline**: two layers architecture: a Bidirectional LSTM layer and a Dense/Fully-Connected layer on top; the choice of hyper-parameters is yours.\n",
    "\n",
    "**Architectures**: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and adding an additional dense layer; do not mix these variantions.\n",
    "\n",
    "**Training and Experiments**: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "**Evaluation**: in the end, only the two best models of your choice (according to the validation set) must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech. DO NOT CONSIDER THE PUNCTUATION CLASSES.\n",
    "\n",
    "**Metrics**: the metric you must use to evaluate your final model is the F1-macro, WITHOUT considering punctuation/symbols classes; during the training process you can use accuracy because you can't use the F1 metric unless you use a single (gigantic) batch because there is no way to aggregate \"partial\" F1 scores computed on mini-batches.\n",
    "\n",
    "**Discussion and Error Analysis** : verify and discuss if the results on the test sets are coherent with those on the validation set; analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "**Report**: you are asked to deliver the code of your experiments and a small pdf report of about 2 pages; the pdf must begin with the names of the people of your team and a small abstract (4-5 lines) that sums up your findings.\n",
    "\n",
    "# Out Of Vocabulary (OOV) terms\n",
    "\n",
    "How to handle words that are not in GloVe vocabulary?\n",
    "You can handle them as you want (random embedding, placeholder, whatever!), but they must be STATIC embeddings (you cannot train them).\n",
    "\n",
    "But there is a very important caveat! As usual, the element of the test set must not influence the elements of the other splits!\n",
    "\n",
    "So, when you compute new embeddings for train+validation, you must forget about test documents.\n",
    "The motivation is to emulate a real-world scenario, where you select and train a model in the first stage, without knowing nothing about the testing environment.\n",
    "\n",
    "For implementation convenience, you CAN use a single vocabulary file/matrix/whatever. The principle of the previous point is that the embeddings inside that file/matrix must be generated independently for train and test splits.\n",
    "\n",
    "Basically in a real-world scenario, this is what would happen:\n",
    "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
    "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
    "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
    "4. Training of the model(s)\n",
    "5. Compute embeddings for terms OOV2 of the validation split \n",
    "6. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
    "7. Validation of the model(s)\n",
    "8. Compute embeddings for terms OOV3 of the test split \n",
    "9. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2+OOV3\n",
    "10. Testing of the final model\n",
    "\n",
    "In this case, where we already have all the documents, we can simplify the process a bit, but the procedure must remain rigorous.\n",
    "\n",
    "1. Starting vocabulary V1 (in this assignment, GloVe vocabulary)\n",
    "2. Compute embeddings for terms out of vocabulary V1 (OOV1) of the training split \n",
    "3. Add embeddings to the vocabulary, so to obtain vocabulary V2=V1+OOV1\n",
    "4. Compute embeddings for terms OOV2 of the validation split \n",
    "5. Add embeddings to the vocabulary, so to obtain vocabulary V3=V1+OOV1+OOV2\n",
    "6. Compute embeddings for terms OOV3 of the test split \n",
    "7. Add embeddings to the vocabulary, so to obtain vocabulary V4=V1+OOV1+OOV2\n",
    "8. Training of the model(s)\n",
    "9. Validation of the model(s)\n",
    "10. Testing of the final model\n",
    "\n",
    "Step 2 and step 6 must be completely independent of each other, for what concerns the method and the documents. But they can rely on the previous vocabulary (V1 for step 2 and V3 for step 6)\n",
    "THEREFORE if a word is present both in the training set and the test split and not in the starting vocabulary, its embedding is computed in step 2) and it is not considered OOV anymore in step 6).\n",
    "\n",
    "# Report\n",
    "The report must not be just a copy and paste of graphs and tables!\n",
    "\n",
    "The report must not be longer than 2 pages and must contain:\n",
    "* The names of the member of your team\n",
    "* A short abstract (4-5 lines) that sum ups everything\n",
    "* A general description of the task you have addressed and how you have addressed it\n",
    "* A short description of the models you have used\n",
    "* Some tables that sum up your findings in validation and test and a discussion of those results\n",
    "* The most relevant findings of your error analysis\n",
    "\n",
    "# Evaluation Criterion\n",
    "\n",
    "The goal of this assignment is not to prove you can find best model ever, but to face a common task, structure it correctly, and follow a correct and rigorous experimental procedure.\n",
    "In other words, we don't care if you final models are awful as long as you have followed the correct procedure and wrote a decent report.\n",
    "\n",
    "The score of the assignment will be computed roughly as follows\n",
    "* 1 point for the general setting of the problem\n",
    "* 1 point for the handling of OOV terms\n",
    "* 1 point for the models\n",
    "* 1 point for train-validation-test procedure\n",
    "* 2 point for the discussion of the results, error analysis, and report\n",
    "\n",
    "This distribution of scores is tentative and we may decide to alter it at any moment.\n",
    "We also reserve the right to assign a small bonus (0.5 points) to any assignment that is particularly worthy. Similarly, in case of grave errors, we may decide to assign an equivalent malus (-0.5 points).\n",
    "\n",
    "# Contacts\n",
    "\n",
    "In case of any doubt, question, issue, or help we highly recommend you to check the [course useful material](https://virtuale.unibo.it/pluginfile.php/1036039/mod_resource/content/2/NLP_Course_Useful_Material.pdf) for additional information, and to use the Virtuale forums to discuss with other students.\n",
    "\n",
    "You can always contact us at the following email addresses. To increase the probability of a prompt response, we reccomend you to write to both the teaching assistants.\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it\n",
    "\n",
    "\n",
    "# FAQ\n",
    "* You can use a non-trainable Embedding layer to load the glove embeddings\n",
    "* You can use any library of your choice to implement the networks. Two options are tensorflow/keras or pythorch. Both these libraries have all the classes you need to implement these simple architectures and there are plenty of tutorials around, where you can learn how to use them.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}